{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from generators import *\n",
    "from helpers import *\n",
    "from networks import *\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters\n",
    "BATCH_SIZE = 8\n",
    "epochs = 10\n",
    "sampling = [0,0,5,5,10,5] #0 bckg, 1 bladder, 2 kidneys, 3 liver, 4 pancreas, 5 spleen\n",
    "outpath = '/home/eva/Desktop/research/PROJEKT2-DeepLearning/AnatomyAwareDL/Results/'\n",
    "outpath_nets = outpath+'Networks/'\n",
    "\n",
    "#set dataset and loaders\n",
    "subjekti = glob.glob('/home/eva/Desktop/research/PROJEKT2-DeepLearning/AnatomyAwareDL/Data/TRAINdata/sub*'); subjekti.sort()\n",
    "labele = glob.glob('/home/eva/Desktop/research/PROJEKT2-DeepLearning/AnatomyAwareDL/Data/TRAINdata/lab*'); labele.sort()\n",
    "\n",
    "subbatch = sum(sampling)\n",
    "#dataset = POEMDataset(subjekti[0:10], labele[0:10], 25, 9, sampling) #my_collate\n",
    "dataset = POEMDatasetMultiInput(subjekti[0:10], labele[0:10], sampling, 25, channels=[0,1], subsample=True,\n",
    "                 channels_sub=[0,1])\n",
    "train_loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=multi_input_collate) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer, network and set parameters\n",
    "network_type = \"DeepMedOrig\"\n",
    "#net = OnePathway(in_channels=4, num_classes=6, dropoutrateCon=0.2, dropoutrateFC=0.5)\n",
    "net = DualPathway(in_channels_orig=2, in_channels_subs=2, num_classes=6, dropoutrateCon=0.2, dropoutrateFC=0.5, nonlin=\"prelu\")\n",
    "net = net.float()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "napaka = nn.CrossEntropyLoss(weight=None, ignore_index=1, reduction='mean') #weight za weightedxentropy, ignore_index ce ces ker klas ignorat.\n",
    "\n",
    "log_interval = 1 #na kolko batchev reportas.\n",
    "val_interval = 5 #na kolko epoch delas validation.\n",
    "\n",
    "# train on cuda if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "# training loop:\n",
    "train_losses = []\n",
    "training_Dice = []\n",
    "val_losses = []\n",
    "val_Dice = []\n",
    "\n",
    "vsehslik = len(train_loader.dataset)*subbatch\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Train Epoch: {}'.format(epoch))\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    epoch_dice = 0.0\n",
    "\n",
    "  \n",
    "  #  tqdm_iter = tqdm_(enumerate(train_loader), total=len(train_loader), desc=\">>Training: \")\n",
    "    for batch_idx, (notr, target) in enumerate(train_loader): #tqdm_iter:\n",
    "        notr, target = [torch.as_tensor(notri, device=device).float() for notri in notr], torch.as_tensor(target, device=device)\n",
    "        # print(len(notr))\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        ven = net(*notr)\n",
    "        \n",
    "        loss = napaka(ven, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        dice = dice_coeff(nn.Softmax(dim=1)(ven), target, nb_classes=6, weights=np.array([0,1,1,1,1,1]))\n",
    "        dice_organs = dice_coeff_per_class(nn.Softmax(dim=1)(ven), target, nb_classes=6)\n",
    "        epoch_dice += dice_organs.sum(0).squeeze()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss += loss.item() * ven.shape[0] #or loss.detach() - detach disables differentiation through here (?)\n",
    "        if batch_idx%log_interval==0:\n",
    "            print('[{:.0f}%]\\tAccumulated batch loss: {:.6f}\\tBatch generalized Dice: {:.6f}'.format(\n",
    "                                                100.*batch_idx/len(train_loader),\n",
    "                                                running_loss/(batch_idx+1), dice.sum()/len(notr[0])))\n",
    "            print('Dice (batch averages) by organ: ', dice_organs.mean(0).data.numpy())\n",
    "\n",
    "    if (epoch+1)%val_interval==0: #TODO add validation\n",
    "        #with torch.no_grad():\n",
    "            #for x_val, y_val in val_loader:\n",
    "             #   x_val = x_val.to(device)\n",
    "            #    y_val = y_val.to(device)\n",
    "           #     net.eval()\n",
    "          #      yhat = model(x_val)\n",
    "         #       val_loss = loss_fn(y_val, yhat)\n",
    "        #        val_losses.append(val_loss.item())\n",
    "        pass\n",
    "                #do validation, save results. PRint.\n",
    "\n",
    "    epoch_loss = epoch_loss/vsehslik\n",
    "    epoch_dice = epoch_dice.squeeze()/vsehslik\n",
    "    print('>>> Epoch {} finished. Averaged loss: {:.6f}, average Dices: {} \\n'.format(\n",
    "        epoch, epoch_loss, epoch_dice.data.numpy()))\n",
    "    train_losses.append(epoch_loss)\n",
    "    training_Dice.append(epoch_dice.data.numpy())\n",
    "\n",
    "    \n",
    "today = datetime.today()\n",
    "now = datetime.now() \n",
    "timestamp = today.strftime(\"_%m%d%Y\")+now.strftime(\"_%H%M%S\")\n",
    "\n",
    "print(\"Training finished. Saving metrics...\")\n",
    "dejta = np.column_stack([np.array(train_losses), np.array(training_Dice)])\n",
    "df = pd.DataFrame(data=dejta,    # values\n",
    "    columns=np.array(['Loss', 'Dice Bckg', 'Dice Bladder', 'Dice Kidneys', 'Dice Liver', 'Dice Pancreas', 'Dice Spleen']))\n",
    "df.to_csv(outpath_nets+network_type+timestamp+'_DiceAndLoss.csv')\n",
    "\n",
    "#saving (and reloading) of network\n",
    "print(\"Saving trained network...\")\n",
    "torch.save(net, outpath_nets+network_type+timestamp)\n",
    "#to remember what network was trained here, let's save parameters for a quick lookup:\n",
    "run_dict = {\"networkType\": network_type, \"channels\": dataset.channels, \"segment\":dataset.segment_size,\n",
    "            \"channelsSubsampled\": dataset.channels_sub, \"useSubsampled\": dataset.subsample, \"sampling\": sampling,\n",
    "            \"segment2\": dataset.segment_size2, \"channels2\": dataset.channels2, \"batchsize\": BATCH_SIZE, \"lastEpoch\": epochs}\n",
    "with open(outpath_nets+network_type+timestamp+'.txt', 'w') as f:\n",
    "    print(run_dict, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load network, if you already have a trained version saved somewhere.\n",
    "saved_network = outpath_nets + network_type + timestamp\n",
    "\n",
    "net = torch.load(saved_network)\n",
    "net.to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION ON TEST IMAGES:\n",
    "#-cut them in patches if needed <- do this in advance!!\n",
    "#-inference on patches\n",
    "#-sew patches back together\n",
    "#So far, Dices were calc. on patches, so they don't say much... Calc again on sewn pics!\n",
    "\n",
    "##############\n",
    "test_subjekti = subjekti[0:5]\n",
    "test_labele = labele[0:5]\n",
    "patchsize = 52\n",
    "overlap = 8\n",
    "##############\n",
    "need_to_cut = True #change to True if any of test data parameters are changed.\n",
    "do_inference = True\n",
    "##############\n",
    "\n",
    "if need_to_cut:\n",
    "    test_list = cut_patches(test_subjekti, patchsize, overlap*2, channels=4, \n",
    "                            outpath=outpath, subsampledinput=True)\n",
    "    # best to always run 'cut_patches' with 4channels, since data loader itself takes care of cases \n",
    "    # with using different channels.\n",
    "else:\n",
    "    test_list = glob.glob(outpath+\"subj_*[0-9].npy\")\n",
    "\n",
    "##############\n",
    "test_dataset = POEMDatasetTEST(test_list, channels=[0,1], subsampled=True, channels_sub=[0,1], input2=None, channels2=None)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=test_collate)\n",
    "##############\n",
    "\n",
    "test_losses = []\n",
    "test_dices = []\n",
    "net.eval()\n",
    "i=0\n",
    "if do_inference:\n",
    "    for slike, names in test_loader:\n",
    "        print(f\"Doing test inference, batch nr. {i+1}/{len(test_loader)}\")\n",
    "        i+=1\n",
    "        slike = [torch.as_tensor(slika, device=device) for slika in slike]  #this even needed? only to_device?\n",
    "  #     print(\"len slike: \", len(slike))\n",
    "  #     print(\"shape slike[0]: \", slike[0].shape)\n",
    "        segm = net(*slike)\n",
    "        #save all processed patches temporarily; without doing softmax, since you need one-hot for dice etc later\n",
    "        for patchnr, name in enumerate(names):\n",
    "            np.save(name, np.squeeze(segm[patchnr,...].detach().numpy()))\n",
    "            #ce ne dam detach se kurac prtozi: RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.\n",
    "\n",
    "\n",
    "#after inference is done on entire dataset, glue temporarily saved ndarrays, evaluate Dice++, save pngs.\n",
    "vsitestirani = []\n",
    "for subj in test_labele:\n",
    "    nr = re.findall(r'.*label([0-9]*)\\.pickle', subj)\n",
    "    print(\"Saving tests on subject nr \", nr)\n",
    "    vsitestirani.append(nr[0])\n",
    "    segmentacija = torch.from_numpy(\n",
    "        glue_patches(nr[0], outpath, patchsize, overlap, nb_classes=6)\n",
    "        ) #glues one person, saves result, returns numpy one one-hot.\n",
    "    \n",
    "    tarca = pickle.load(open(subj, 'rb'))\n",
    "    tarca = torch.from_numpy(tarca[np.newaxis, overlap:-overlap, overlap:-overlap, overlap:-overlap])\n",
    "\n",
    "    test_loss = napaka(segmentacija, tarca.long())  #napaka needs onehot, does softmax inside.\n",
    "    test_losses.append(test_loss.item())\n",
    "    dajs = dice_coeff_per_class(nn.Softmax(dim=1)(segmentacija), tarca, nb_classes=6) #Dice expects softmax!\n",
    "    test_dices.append(dajs.data.numpy().squeeze())\n",
    "\n",
    "    #reset for counting and new subject:\n",
    "    print('{}: Loss {:.4f}, \\t Dices {}'.format(nr, test_loss.item(), dajs.data.numpy()))\n",
    "\n",
    "print(\"Testing finished. Saving metrics...\")\n",
    "dejta = np.column_stack([np.array(test_losses), np.array(test_dices)])\n",
    "df = pd.DataFrame(data=dejta,    # values\n",
    "                  index=vsitestirani,\n",
    "                columns=np.array(['Loss', 'Dice Bckg', 'Dice Bladder', 'Dice Kidneys', 'Dice Liver', 'Dice Pancreas', 'Dice Spleen']))\n",
    "df.to_csv(outpath_nets+network_type+timestamp_'_DiceAndLoss_Test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
